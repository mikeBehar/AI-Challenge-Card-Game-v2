# AI CHALLENGE CARD GAME - MASTER GRADING RUBRIC

## Overview
This rubric provides standardized grading criteria for all four types of challenge cards in the AI Challenge Card Game. Each card type is worth 10 points maximum. Use this rubric to ensure consistent, fair evaluation across all teams and card types.

---

## CARD TYPE 1: PROMPT CARDS (Mini-Case Studies)

### Description:
Teams receive a business scenario and must create an effective prompt to solve the problem using AI.

### Grading Criteria (10 points total):

**9-10 Points (Excellent):**
- Prompt is clear, specific, and actionable
- Includes relevant context from the case study
- Specifies desired output format or constraints
- Anticipates potential AI limitations and addresses them
- Shows understanding of the business problem
- Prompt would likely produce useful results

**7-8 Points (Good):**
- Prompt addresses the core problem
- Includes most necessary context
- Output expectations are clear
- Minor gaps in specificity or constraints
- Would produce somewhat useful results

**5-6 Points (Fair):**
- Prompt attempts to address the problem
- Missing important context or constraints
- Vague output expectations
- May produce unreliable or incomplete results
- Shows partial understanding of the problem

**0-4 Points (Poor):**
- Prompt is too vague or generic
- Misunderstands the business problem
- Missing critical context
- No clear output expectations
- Would not produce useful results

### Example Excellent Response:
**Case: Customer service chatbot for returns**
*"You are a customer service assistant for an online retailer. A customer contacts you about returning a jacket purchased 25 days ago (our return window is 30 days). They say it doesn't fit and they want a refund. Respond professionally and helpfully. Include: (1) confirmation we can process the return, (2) clear step-by-step return instructions, (3) estimated refund timeline, (4) ask if they'd like to exchange for a different size instead. Keep response under 150 words and maintain friendly, empathetic tone."*

### Example Poor Response:
**Same case:**
*"Help the customer return their jacket."*

---

## CARD TYPE 2: DEBATE CARDS

### Description:
Teams must argue for or against a specific AI application or policy position within a business/ethical context.

### Grading Criteria (10 points total):

**9-10 Points (Excellent):**
- Clear, well-structured argument with 3+ strong supporting points
- Anticipates and addresses counterarguments
- Uses specific examples or evidence
- Demonstrates understanding of nuance and tradeoffs
- Professional, persuasive presentation
- Shows critical thinking about implications

**7-8 Points (Good):**
- Clear argument with 2-3 supporting points
- Some engagement with counterarguments
- Uses examples, though may be generic
- Generally persuasive
- Shows reasonable understanding of the issue

**5-6 Points (Fair):**
- Basic argument present but underdeveloped
- 1-2 weak supporting points
- Little engagement with counterarguments
- Few or no specific examples
- Surface-level understanding of the issue

**0-4 Points (Poor):**
- Unclear or incoherent argument
- No supporting evidence
- Does not engage with the assigned position
- Misunderstands the debate topic
- No demonstration of critical thinking

### Example Excellent Response:
**Debate: "Companies should be required to disclose when customers are interacting with AI rather than humans"**

*"We strongly support mandatory AI disclosure for three key reasons:*

*First, informed consent is a fundamental consumer right. When customers share personal information or make decisions based on advice, they deserve to know whether they're interacting with a human who can exercise judgment or an AI following patterns. Studies show people communicate differently with AI versus humans, and this affects the quality of support they receive.*

*Second, disclosure enables appropriate expectations. Customers can adjust their communication style, know when to escalate to humans for complex issues, and understand the limitations of automated systems. This actually improves satisfaction—frustration comes from discovering after the fact they were talking to a bot.*

*Third, transparency builds trust and accountability. Companies that hide AI usage signal they don't trust their own systems. When something goes wrong, customers should know whether the error was algorithmic or human to pursue appropriate remedies.*

*Opponents argue disclosure will make customers resistant to AI, but research shows people accept AI assistance when it's transparent and appropriate. The solution isn't deception—it's building AI worth disclosing."*

### Example Poor Response:
**Same debate:**
*"Yes, companies should tell people because it's the right thing to do and honesty is important."*

---

## CARD TYPE 3: PREDICTION CARDS (NEW)

### Description:
Teams receive a scenario about a proposed or newly deployed AI system. They must predict what could go wrong by identifying the top 3 risks with reasoning, BEFORE seeing the actual outcomes.

### Grading Criteria (10 points total):

**9-10 Points (Excellent):**
- Identifies 2-3 sophisticated, specific risks
- Explains WHY each risk matters (mechanisms, not just labels)
- Shows understanding of systemic issues (bias, proxy variables, feedback loops)
- Reasoning demonstrates critical thinking about data, context, or power dynamics
- Risks are plausible and well-connected to scenario details
- Goes beyond obvious to identify subtle or complex failure modes

**7-8 Points (Good):**
- Identifies 1-2 clear, specific risks with solid reasoning
- Explanations show understanding of how AI systems can fail
- May not reach the deepest level of analysis but demonstrates critical thinking
- Risks are relevant and well-articulated
- Shows engagement with scenario details

**5-6 Points (Fair):**
- Identifies 1-2 risks but reasoning is superficial or generic
- Uses broad labels like "bias" without explaining mechanisms
- Limited connection to specific scenario details
- Shows awareness of issues but not deep understanding
- Obvious risks only, missing nuance

**0-4 Points (Poor):**
- No specific risks identified or misunderstands scenario
- Generic concerns like "what if it's wrong" without elaboration
- No reasoning provided for identified risks
- Risks are implausible or don't connect to scenario
- Demonstrates lack of engagement with material

### Evaluation Notes:
- Award partial credit for identifying correct risk even if reasoning is incomplete
- Value depth over breadth—1 sophisticated risk > 3 superficial ones
- Consider whether team anticipates issues that actually materialized
- Recognize when teams identify risks not in answer key but still valid

### Example Excellent Response:
**Scenario: AI resume screening trained on historical hires**

*"Three major risks concern us:*

*First, historical bias in training data. If the company's past hires were predominantly from certain demographic groups due to past discrimination, the AI will learn those patterns and perpetuate them. The algorithm will treat past hiring patterns as 'success' rather than questioning whether historical hires represent the full range of qualified candidates.*

*Second, proxy variables for protected characteristics. Even without explicitly using gender or race, the AI might pick up on signals like school names, activity descriptions (women's organizations), or gaps in employment (often due to caregiving responsibilities disproportionately affecting women). The algorithm learns discrimination through correlation without any explicit bias coding.*

*Third, inability to identify potential over credentials. Historical 'success' reflects who got hired and stayed, not who could have succeeded if given the chance. The system will favor candidates who match past patterns rather than identifying talented people from non-traditional backgrounds who might bring new perspectives."*

### Example Poor Response:
**Same scenario:**
*"It might be biased. It could make mistakes. People might not like it."*

---

## CARD TYPE 4: RISK ASSESSMENT CARDS (NEW)

### Description:
Teams must conduct comprehensive risk assessment using the 6-part AI Evaluation Framework. They identify risks in ALL 6 categories and rate severity (High/Medium/Low) for each risk.

**The 6 Framework Categories:**
1. Data Quality Risks
2. Transparency & Explainability Risks
3. Testing & Validation Risks
4. Governance & Oversight Risks
5. Scope & Safeguards Risks
6. Context & Equity Risks

### Grading Criteria (10 points total):

**9-10 Points (Excellent):**
- Identifies 12+ specific risks across ALL 6 categories (2+ risks per category)
- Severity ratings are appropriate and well-justified
- Demonstrates deep understanding of how risks interact and compound
- Shows sophisticated analysis of systemic issues
- Distinguishes between different types of risks within categories
- For deployed systems: distinguishes materialized risks from ongoing risks
- Applies framework terminology correctly and meaningfully

**7-8 Points (Good):**
- Identifies 8-11 risks across most categories (may have 1 weak category)
- Severity ratings are generally reasonable
- Shows solid understanding of framework application
- May miss some subtle risks but identifies major concerns
- Good coverage across categories even if not comprehensive
- Demonstrates competent critical thinking

**5-6 Points (Fair):**
- Identifies 5-7 risks but coverage is uneven (may miss 2-3 categories entirely)
- Some risks may be repetitive or miscategorized
- Severity ratings are inconsistent or poorly justified
- Surface-level analysis without deep understanding
- May list risks without explaining why they matter
- Shows partial framework understanding

**0-4 Points (Poor):**
- Identifies fewer than 5 risks
- Missing 3+ categories entirely
- Risks are vague, generic, or misapplied
- Severity ratings absent or nonsensical
- Demonstrates misunderstanding of framework
- No evidence of systematic thinking

### Evaluation Notes:
- Each category should have at least 1 risk for full credit
- Quality matters more than quantity, but comprehensive coverage is expected
- Severity ratings: High = serious harm/clear violation, Medium = significant concern, Low = minor issue
- Give credit for identifying valid risks not in answer key
- For deployed systems, recognize distinction between "materialized" and "present" risks
- Framework application is as important as risk identification

### Example Excellent Response (Partial):
**Scenario: Healthcare algorithm using cost as proxy for health needs**

*"DATA QUALITY RISKS:*
- *Historical access inequality [HIGH]: Healthcare costs reflect who accessed care historically, not who needed care. Black patients facing barriers (discrimination, insurance gaps, medical mistrust) accumulated lower costs despite equal or greater illness.*
- *Proxy variable encodes bias [HIGH]: Cost isn't neutral—it carries all the systemic barriers that prevented equal access. Algorithm learns 'costs less = needs less' without understanding causation.*
- *Selection bias in training [MEDIUM]: Training only on who received care misses those who needed care but couldn't access it.*

*TRANSPARENCY & EXPLAINABILITY RISKS:*
- *Cannot audit for fairness [HIGH]: Without explainability, impossible to verify whether racial disparities reflect actual health needs or algorithmic bias.*
- *Patients cannot challenge decisions [HIGH]: When flagged as 'low risk' despite being sick, patients have no recourse or explanation.*

*TESTING & VALIDATION RISKS:*
- *No pre-deployment demographic testing [HIGH]: System launched without verifying equal accuracy across racial groups.*
- *Wrong success metric [HIGH]: System optimized for cost prediction, not equitable care allocation.*

*[Continues through all 6 categories...]*"

### Example Poor Response (Partial):
**Same scenario:**

*"DATA QUALITY RISKS:*
- *Bad data [MEDIUM]*

*TRANSPARENCY:*
- *Not transparent [HIGH]*

*TESTING:*
- *Wasn't tested enough [LOW]*"

---

## WEIGHTING & SCORING PHILOSOPHY

### Point Values:
All card types are worth **10 points maximum** to maintain consistency and fairness.

### Why Equal Weighting?
- **Complexity varies within types, not between types**: A sophisticated debate can be as challenging as a sophisticated risk assessment
- **Different skills, equal value**: Prompt writing, argumentation, prediction, and systematic analysis are all valuable professional skills
- **Fairness**: Students shouldn't be penalized for drawing certain card types
- **Flexibility**: Instructor can adjust difficulty through card selection rather than point values

### When to Use Different Weights (Optional):
If you choose to weight card types differently for a specific class session or assessment:
- **Risk Assessments (1.5x)**: If emphasizing systematic framework application
- **Debates (1.5x)**: If focusing on argumentation and persuasion skills
- **Predictions (1.2x)**: If rewarding anticipatory thinking
- **Prompts (1.0x baseline)**: Standard weight

### Grading Consistency Tips:
1. **Read answer key first**: Understand the range of acceptable responses
2. **Calibrate early**: Grade first few teams together to establish standards
3. **Be consistent across teams**: Same standard for all teams in same session
4. **Consider partial credit**: Award points for partially correct or well-reasoned responses
5. **Real-time feedback**: Give brief verbal feedback when announcing scores
6. **Document decisions**: Note why you gave specific scores for future reference

---

## COMMON GRADING MISTAKES TO AVOID

### Mistake 1: Rewarding Length Over Quality
- **Wrong**: Giving high scores just because response is long
- **Right**: Evaluate depth of thinking, not word count
- Short, insightful responses can score 9-10 points

### Mistake 2: Penalizing Valid Alternative Perspectives
- **Wrong**: Deducting points because team's risk isn't in answer key
- **Right**: Evaluate reasoning quality; answer keys aren't exhaustive

### Mistake 3: All-or-Nothing Grading
- **Wrong**: Giving 10 points if perfect, 5 points if not
- **Right**: Use full scale—reward progression and partial understanding

### Mistake 4: Inconsistent Standards Between Teams
- **Wrong**: Being harder on later teams after calibrating expectations
- **Right**: Grade all teams against same written rubric

### Mistake 5: Not Reading Full Response
- **Wrong**: Skimming and scoring based on first impression
- **Right**: Read complete response before assigning score

### Mistake 6: Severity Rating Perfectionism (Risk Assessments)
- **Wrong**: Deducting heavily if severity rating differs from answer key
- **Right**: Accept reasonable variation in severity if justified; focus on risk identification

---

## EXAMPLES OF EXCELLENT VS. POOR RESPONSES (By Card Type)

### PROMPT CARD EXAMPLES

**Scenario**: Your company wants to use AI to generate product descriptions from technical specifications.

**Excellent (9-10 points):**
*"You are a product copywriter for a consumer electronics website. Convert the following technical specifications into an engaging product description for online shoppers:*

*[Technical specs would be pasted here]*

*Requirements:*
- *Write 3-4 paragraphs (150-200 words total)*
- *Lead with key benefits customers care about (performance, ease of use, value)*
- *Translate technical jargon into everyday language*
- *Include a compelling call-to-action*
- *Maintain enthusiastic but honest tone (no exaggeration or false claims)*
- *Highlight 2-3 key features that differentiate from competitors*
- *Format with short paragraphs for easy online reading*

*Avoid:*
- *Making unverifiable claims ('best in class,' 'revolutionary')*
- *Technical specifications without context*
- *Excessive use of buzzwords*"

**Poor (0-4 points):**
*"Write a product description."*

---

### DEBATE CARD EXAMPLES

**Debate**: "AI-generated art should not be eligible for copyright protection"

**Excellent (9-10 points):**
*"We argue that AI-generated art should NOT receive copyright protection for three critical reasons that balance innovation incentives with fundamental copyright principles:*

*First, copyright law requires human authorship. The Copyright Office has consistently held that works must be 'created by a human being' because copyright rewards human creative choices, judgment, and expression. AI doesn't make creative decisions—it executes mathematical patterns from training data. Granting AI art copyright would fundamentally redefine authorship to include non-sentient processes, opening doors to copyrighting natural formations or random processes.*

*Second, AI training data often includes copyrighted works without permission. Allowing AI outputs to be copyrighted while the system was trained on others' protected works creates perverse incentives: profit from using copyrighted training data without compensation, then lock up the outputs. This rewards free-riding on human creativity.*

*Third, the economics don't require copyright protection. Unlike human artists who need copyright to earn livelihoods, AI companies are already massively profitable. They don't need additional monopoly rights as incentive to create—the market for AI tools is booming without output protection. Copyright's purpose is to incentivize creation that wouldn't otherwise occur; AI generation happens regardless.*

*Opponents argue denying copyright will stifle AI innovation, but this confuses tool protection with output protection. AI systems themselves may be patented or protected as trade secrets. The question is whether outputs deserve decades-long monopolies. Human-AI collaborative works can still receive copyright for the human creative contribution. Denying pure AI output protection simply means these works enter the public domain immediately, enriching our shared culture rather than locking it behind corporate control."*

**Poor (0-4 points):**
*"AI art shouldn't get copyright because it's not real art and computers aren't creative."*

---

### PREDICTION CARD EXAMPLES

**Scenario**: School district using AI to recommend academic tracks for 5th graders based on test scores, behavior records, and parent income.

**Excellent (9-10 points):**
*"This system raises three critical risks:*

*1. Self-fulfilling prophecy through historical bias: The training data shows which track past students were in and their outcomes, but this reflects past tracking decisions—which we know are biased by race and class—not student potential. The AI will learn to replicate discriminatory tracking rather than predict actual student ability. Students placed in lower tracks receive fewer resources and opportunities, ensuring the 'prediction' comes true regardless of actual potential.*

*2. Poverty as permanent penalty: Using parent income and behavior records as risk factors punishes children for circumstances beyond their control. Behavior records reflect racial bias in school discipline—Black students suspended more for same behaviors. Free lunch status reflects family economics, not student capability. The algorithm hardwires socioeconomic status into educational tracking, preventing upward mobility.*

*3. Loss of late bloomers: Fifth grade is too early to determine academic trajectory. Developmental differences, family circumstances, and late maturation mean many successful adults struggled in elementary school. AI-driven tracking at age 10 cements opportunities based on a snapshot in time, missing students who need more time or different support to excel."*

**Poor (0-4 points):**
*"The AI might make mistakes and put kids in the wrong classes. It might not be fair to everyone."*

---

### RISK ASSESSMENT CARD EXAMPLES

**Scenario**: Transit authority using AI to schedule bus maintenance, resulting in more frequent service disruptions in low-income neighborhoods.

**Excellent (9-10 points - Partial Example):**

*"DATA QUALITY RISKS:*
- *Historical maintenance bias [HIGH - MATERIALIZED]: Training data reflects past maintenance priorities; wealthier neighborhoods may have received better service historically, contaminating 'normal' baseline*
- *Route difficulty confounding [HIGH - MATERIALIZED]: Low-income areas often have worse roads (fewer infrastructure repairs), causing legitimate increased wear—but AI learns geography as risk rather than addressing infrastructure inequality*
- *Driver behavior data misleading [MEDIUM]: 'Aggressive driving' may reflect route demands (city driving, tight schedules) not driver choices—penalizes drivers assigned challenging routes*

*TRANSPARENCY & EXPLAINABILITY RISKS:*
- *Community cannot verify fairness [HIGH]: Affected neighborhoods cannot audit whether service disruptions are mechanically necessary or algorithmically biased*
- *Drivers cannot challenge assessments [MEDIUM - MATERIALIZED]: If driving affects maintenance schedule, drivers deserve explanation and appeal*

*TESTING & VALIDATION RISKS:*
- *No geographic equity testing [HIGH]: System launched without testing whether predictions varied by neighborhood wealth or road quality*
- *No validation of alternative hypothesis [MEDIUM]: Did anyone test whether disparities reflect infrastructure inequality rather than bus condition?*

*[Continues through all 6 categories with similar depth and specificity...]*"

**Poor (0-4 points):**

*"DATA:*
- *Bad data [HIGH]*
- *Not enough data [MEDIUM]*

*TRANSPARENCY:*
- *Can't see how it works [HIGH]*

*TESTING:*
- *Needs more testing [MEDIUM]*"

---

## FINAL GRADING REMINDERS

### For Real-Time Game Evaluation:
1. **Read team response completely** before scoring
2. **Consult answer key** but don't treat it as only acceptable answer
3. **Consider context**: Did team have enough time? Was card particularly difficult?
4. **Announce score with brief justification**: "Team 3 receives 8 points—you identified two strong risks with good reasoning, but missed the proxy variable issue we discussed"
5. **Be consistent**: Don't change standards mid-game

### For Learning (Not Just Grading):
- **Reveal answer after scoring**: Show what teams missed
- **Highlight excellent responses**: "Team 2 had a great insight about..."
- **Connect to course themes**: "This shows why we emphasize..."
- **Encourage improvement**: "Next time, try to..."

### Handling Disputes:
- **Listen to team's reasoning**: "Tell me your thinking..."
- **Refer to rubric**: "Based on the criteria..."
- **Be fair but firm**: Scores are final once announced
- **Learn for next time**: If many teams struggle, card may be too difficult

---

**END OF MASTER RUBRIC**
