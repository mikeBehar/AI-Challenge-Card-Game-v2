# Master Grading Rubrics
**Course**: Generative AI for Business and Creative Professionals  
**Institution**: CT State Norwalk Community College  
**Instructor**: Mike Behar

---

## Purpose of This Document

This document contains all grading rubrics used throughout the course. Each rubric is referenced in specific assignments so students know exactly how their work will be assessed.

**Core Philosophy**: We assess the quality of thinking and AI collaboration skills, not agreement with instructor views or domain expertise (finance, ethics, etc.).

---

## Table of Contents

1. [Prompt Quality Rubric](#1-prompt-quality-rubric)
2. [Written Analysis Rubric](#2-written-analysis-rubric)
3. [AI Collaboration Rubric](#3-ai-collaboration-rubric)
4. [Deliberation Participation Rubric](#4-deliberation-participation-rubric)
5. [Business Acumen Rubric](#5-business-acumen-rubric)
6. [Ethical Awareness Rubric](#6-ethical-awareness-rubric)
7. [Self-Assessment Checklists](#7-self-assessment-checklists)

---

## 1. Prompt Quality Rubric

**Used for**: Pre-meeting assignments (Question 3), AI consultation breaks, any prompt submission

**Total Points**: Varies by assignment (typically 10-35% of total grade)

### Detailed Scoring Guide

| Score | Specificity & Context | Strategic Value | Critical Evaluation | Iterative Thinking |
|-------|----------------------|-----------------|---------------------|-------------------|
| **Exceptional (95-100%)** | Provides comprehensive context including: what you need to know, why it matters, constraints/requirements, desired output format. Anticipates AI's information needs. | Asks AI to help you think critically: identify blind spots, challenge assumptions, generate alternatives you haven't considered, reveal trade-offs. Goes beyond just getting information. | Explicitly plans to evaluate AI's reasoning: "What assumptions are you making?" "Challenge this conclusion." Shows skepticism and critical thinking. | Demonstrates clear iterative strategy: initial broad question → focused follow-up → probing contradictions → synthesis question. Shows 3+ rounds of refinement. |
| **Excellent (90-94%)** | Clear, focused question with relevant context. Provides enough background that AI can give targeted response. Specifies what kind of answer you need. | Asks for analysis, comparison, or implications rather than just facts. Helps you understand the "why" and "so what?" not just the "what." | Includes at least one follow-up that questions or probes AI's initial answer. Shows you won't blindly accept first response. | Shows 2-3 rounds of questioning, with each follow-up building on or challenging previous AI response. Clear progression of thought. |
| **Proficient (80-89%)** | States question clearly with basic context. AI can understand what you're asking but might need more detail to give best answer. | Asks for something that could inform your decision (not just trivia). Has business relevance even if not deeply strategic. | Acknowledges (explicitly or implicitly) that you'll need to evaluate AI's answer. May include cautious language like "help me understand" rather than "tell me." | Has at least one follow-up question showing you engaged with AI's response. May be simple but shows iterative thinking. |
| **Developing (70-79%)** | Question is understandable but lacks context or specificity. AI might give generic answer because it doesn't know your specific situation. | Asks for information that's potentially relevant but doesn't clearly connect to your decision-making need. Feels like research rather than analysis. | Takes AI's answer at face value without indication you'll evaluate it critically. No evidence of skepticism. | Single prompt only, or follow-ups that just ask for "more information" rather than deeper understanding. Limited iteration. |
| **Needs Improvement (60-69%)** | Vague or overly broad question. AI would struggle to give useful answer without asking you clarifying questions. | Asks AI to make your decision ("What should I do?" "Which option is best?") rather than help you think it through. Or asks for irrelevant information. | No indication of critical thinking. Prompt suggests you'll use AI's answer directly without evaluation. | No follow-up questions. One-and-done approach. |
| **Insufficient (Below 60%)** | Unclear what you're asking. Missing basic structure of a question. | No clear connection to course content or decision at hand. Asks AI to do your thinking for you. | Completely delegates judgment to AI. | No iteration whatsoever. |

### Examples with Commentary

#### Exceptional Prompt (95-100%):

> **Prompt**: "I'm evaluating whether a regional bank should deploy an AI loan screening tool that shows 23% auto-decline rate for Black applicants versus 12% for white applicants. The bank's current human process has unknown disparate impact rates - we've never measured it. 
>
> Help me understand: What are three different ways to interpret this disparity? Could some of these explanations be less concerning than others from a fairness perspective? What additional data would help me determine which interpretation is most likely correct?
>
> After you give me those explanations, I want you to challenge your own reasoning - what am I potentially missing by framing the problem this way?"

**Why this is exceptional**:
- ✅ Comprehensive context (specific numbers, comparison point, what's unknown)
- ✅ Asks for multiple interpretations (not just one answer)
- ✅ Explicitly requests AI to challenge itself
- ✅ Shows strategic thinking about what data matters
- ✅ Sets up clear follow-up conversation

#### Excellent Prompt (90-94%):

> **Prompt**: "Compare the total 3-year cost of ownership for Option 1 (buy vendor solution at $850K year 1) versus Option 2 (build internal capability at $1.2M year 1). Include direct costs, opportunity costs, and hidden costs that might not be obvious in the case study. What assumptions matter most in this comparison?"

**Why this is excellent**:
- ✅ Specific question with clear parameters
- ✅ Asks for comprehensive analysis (direct + opportunity + hidden costs)
- ✅ Requests identification of key assumptions
- ✅ Would enable informed decision-making
- ❓ Could be even better by explicitly asking AI to challenge the comparison

#### Proficient Prompt (80-89%):

> **Prompt**: "What are the main privacy risks associated with using facial recognition in retail stores? What regulations might apply?"

**Why this is proficient**:
- ✅ Clear, specific question
- ✅ Relevant to business decision
- ✅ Two-part question shows some depth
- ❌ Lacks context about the specific business situation
- ❌ Doesn't set up follow-up or critical evaluation
- ❌ Could be more strategic (e.g., "How do these risks compare to the revenue benefits?")

#### Developing Prompt (70-79%):

> **Prompt**: "Explain the pros and cons of AI in marketing."

**Why this is developing**:
- ✅ Asks for balanced view (pros and cons)
- ❌ Too broad - which aspects of marketing? What kind of AI?
- ❌ No context about specific business situation
- ❌ Doesn't connect to decision you need to make
- ❌ No indication of follow-up or critical thinking

#### Needs Improvement (60-69%):

> **Prompt**: "Should we use AI for our marketing or not?"

**Why this needs improvement**:
- ❌ Asks AI to make your decision
- ❌ No context provided
- ❌ Binary yes/no when real decisions are more nuanced
- ❌ No evidence you'll evaluate AI's reasoning
- ❌ Doesn't help you think, just asks for an answer

#### Insufficient (Below 60%):

> **Prompt**: "Tell me about AI."

**Why this is insufficient**:
- ❌ Completely vague - AI in what context?
- ❌ No relevance to course or decision
- ❌ Would produce generic, unhelpful response
- ❌ Shows no understanding of how to collaborate with AI

---

## 2. Written Analysis Rubric

**Used for**: Pre-meeting assignment responses (Questions 1, 2, 4), post-deliberation reflections

**Total Points**: Varies by assignment (typically 10 points per question)

### Detailed Scoring Guide

| Score | Case Engagement | Business Acumen | Ethical Awareness | Evidence & Reasoning | Synthesis |
|-------|----------------|-----------------|-------------------|---------------------|-----------|
| **Exceptional (95-100%)** | Deep engagement with specific case data. References numbers, quotes stakeholder perspectives, connects multiple case elements. Shows you read carefully. | Demonstrates sophisticated business thinking: considers opportunity costs, competitive dynamics, organizational capabilities, time horizons. Thinks like a board member. | Integrates ethical considerations as business factors: reputation risk, stakeholder relationships, long-term sustainability. Not separate "ethics section." | Every claim supported by case data or logical reasoning. Acknowledges what's unknown. Addresses counterarguments. | Creates new insights by connecting ideas. Goes beyond case to draw on experience or external knowledge appropriately. Original thinking evident. |
| **Excellent (90-94%)** | References specific case details including key numbers and facts. Shows thorough understanding of scenario. | Strong practical business thinking. Considers multiple factors (cost, risk, time, feasibility). Realistic about implementation. | Recognizes ethical dimensions and treats them as important business considerations. Shows awareness of stakeholder impacts. | Claims generally supported. Uses case data effectively. Reasoning is sound even if not comprehensive. | Integrates multiple perspectives. Makes clear recommendation with reasoning. Shows independent thought. |
| **Proficient (80-89%)** | Demonstrates understanding of case situation. References some specific details but could be more thorough. | Sound business reasoning on main factors. May miss some nuances but gets core issues right. Practical rather than theoretical. | Acknowledges ethical concerns. May treat ethics as separate consideration rather than integrated business factor. | Main points supported but some assertions made without evidence. Logic is followable. | Addresses the question adequately. Shows competent analysis even if not particularly original. |
| **Developing (70-79%)** | General understanding but limited specific case engagement. Could apply to many similar cases. Generic rather than case-specific. | Basic business thinking present but may be simplistic. Misses important factors or doesn't connect implications to decision. | Mentions ethics but superficially. May rely on platitudes rather than analysis of actual impacts. | Some claims unsupported. Reasoning has gaps. May rely more on opinion than analysis. | Answers the question but at surface level. Limited integration of different considerations. |
| **Needs Improvement (60-69%)** | Shows minimal case engagement. Doesn't reference specific details. Response could apply to almost any business scenario. | Weak or confused business reasoning. May focus on irrelevant factors or miss obvious implications. | Ethics largely ignored or treated as checkbox item. Little awareness of stakeholder complexity. | Few claims supported by evidence. Assertions without reasoning. May misunderstand case facts. | Partial answer or misses key aspects of question. Limited analysis. |
| **Insufficient (Below 60%)** | Little evidence of reading case. Generic statements only. | No clear business reasoning. May be completely off-topic. | Ethics not considered or handled inappropriately. | No evidence or reasoning provided. Unsupported opinions only. | Does not address question or shows fundamental misunderstanding. |

### Examples with Commentary

#### Exceptional Written Analysis (95-100%):

> **Question**: "Which option do you recommend and why?"
>
> **Student Response**: "I recommend Option 3 (six-month delay for audit and fixes) despite the competitive pressure for speed. Here's my reasoning:
>
> First, the case reveals Liberty Regional has never measured disparate impact in its lending—meaning the 23% vs 12% auto-decline gap might reflect existing discriminatory practices that the AI made visible rather than created. An audit addresses the root problem, not just the AI symptom. This is fundamentally about organizational accountability.
>
> Second, the financial math favors prudence. Option 1's projected $400K savings is dwarfed by potential legal costs. The case mentions that similar situations have resulted in $5-10M settlements. Even with a 15% probability of litigation (the vendor's estimate), the expected cost of Option 1 is $750K-$1.5M in legal exposure—nearly double the six-month audit cost of $180K.
>
> Third, competitive timing matters less than it appears. The case states that FinTech Solutions' other clients haven't yet captured significant market share with this tool. A six-month delay is painful but not fatal if it results in a sustainable system. The real competitive risk is being the regional bank that becomes the cautionary tale about algorithmic discrimination.
>
> The strongest argument against Option 3 is that six months might not be enough—the audit could reveal problems requiring 12-18 months to fix. I asked Claude to analyze audit timelines for similar cases, and that's a real risk. However, Option 3 gives us a decision point: we can assess audit findings at month 3 and adjust. Option 1 locks us into a potentially broken system immediately.
>
> I'm also weighing Brad's point about vendor dependence versus Jorge's emphasis on community relationships. Both are valid. Option 3 doesn't solve the vendor dependency, but it buys time to evaluate whether building internal capability (Option 5) is truly feasible. The audit might reveal we need capabilities FinTech Solutions can't provide anyway."

**Why this is exceptional**:
- ✅ Cites specific case numbers (23% vs 12%, $400K savings, $180K audit cost)
- ✅ Sophisticated financial reasoning (expected value calculation)
- ✅ Integrates ethical concerns as business risk (not separate)
- ✅ Acknowledges counterarguments (six months might not be enough)
- ✅ Shows engagement with peer deliberation (references Brad and Jorge)
- ✅ Demonstrates use of AI (asked Claude about audit timelines)
- ✅ Original insight (AI made existing bias visible vs. created it)
- ✅ Strategic thinking about decision sequencing

#### Excellent Written Analysis (90-94%):

> **Question**: "Identify three stakeholders most impacted by this decision."
>
> **Student Response**: "The three most impacted groups are:
>
> 1. **Minority loan applicants**: Face 23% auto-decline rate vs. 12% for white applicants under the AI system. This affects their ability to access capital for businesses or homes, potentially widening existing wealth gaps. The case doesn't specify whether human underwriters were better or worse, which is a critical unknown.
>
> 2. **Loan officers**: The case mentions Liberty lost 8 officers in 18 months, partly due to concerns about AI replacing their roles. If we deploy, remaining officers face reduced autonomy and potential job loss. If we don't deploy, they face continued pressure from the efficiency demands that caused the AI consideration in the first place.
>
> 3. **Liberty Regional's reputation**: As a community bank that markets itself on relationships and trust, being labeled as engaging in algorithmic discrimination would be existentially damaging. The Bridgeport community's perception of the bank drives its core business model. Loss of trust would be hard to recover from.
>
> Each group faces both risks and potential benefits depending on which option we choose, which is why this decision is genuinely difficult."

**Why this is excellent**:
- ✅ Specific case references (23% vs 12%, 8 officers lost)
- ✅ Explains actual impacts, not just identifies groups
- ✅ Recognizes uncertainty (unknown human performance)
- ✅ Notes bidirectional impacts (both risks and benefits)
- ✅ Shows understanding of stakeholder complexity
- ❓ Could be exceptional with more depth on opportunity costs or competitive factors

#### Proficient Written Analysis (80-89%):

> **Question**: "What are the main risks of Option 1?"
>
> **Student Response**: "Option 1 (deploy as-is) has three major risks. First is legal risk—if the disparate impact is found to violate fair lending laws, we could face fines and lawsuits. Second is reputational risk—community banks depend on trust and being seen as discriminatory would hurt our customer base. Third is employee morale—some staff are already concerned about the AI, and deploying a biased system could make retention worse.
>
> These risks need to be weighed against the efficiency gains we'd get from faster loan processing and lower costs."

**Why this is proficient**:
- ✅ Identifies relevant risks clearly
- ✅ Shows understanding of case situation
- ✅ Acknowledges trade-offs at the end
- ❌ Doesn't cite specific numbers from case
- ❌ Fairly generic (could apply to many AI deployment cases)
- ❌ Doesn't analyze magnitude of risks or mitigation options

#### Developing Written Analysis (70-79%):

> **Question**: "Should Liberty Regional deploy the AI system?"
>
> **Student Response**: "I don't think they should deploy it yet because there are bias concerns and that's not good for business. Banks need to be fair to all customers. The AI might save money but that's not worth it if people get treated unfairly. They should wait until they can make sure the AI is not biased."

**Why this is developing**:
- ✅ Takes a position
- ✅ Mentions both business and ethical concerns
- ❌ Very general (no case specifics cited)
- ❌ Simplistic reasoning (no nuance about trade-offs)
- ❌ Doesn't engage with specific options or alternatives
- ❌ Could apply to any AI bias case

---

## 3. AI Collaboration Rubric

**Used for**: Evaluating how effectively students partner with AI tools throughout their work

**Total Points**: Typically integrated into prompt rubric (35% of assignments)

### Detailed Scoring Guide

| Score | Question Quality | Tool Selection | Critical Evaluation | Synthesis & Integration | Iterative Learning |
|-------|-----------------|----------------|---------------------|------------------------|-------------------|
| **Exceptional (95-100%)** | Asks sophisticated questions that demonstrate deep thinking. Questions help uncover blind spots, generate alternatives, or challenge assumptions. Strategic about what to ask AI vs. figure out yourself. | Chooses appropriate tool for the task (Claude for analysis, ChatGPT for brainstorming, etc.). Explains why chosen tool fits the need. May use multiple tools strategically. | Actively probes AI's reasoning: "What assumptions are you making?" "What might you be missing?" "Challenge your own conclusion." Shows healthy skepticism. | Seamlessly integrates AI insights into own analysis. AI helps thinking without replacing it. Clear distinction between what AI said vs. own judgment. | Shows clear learning progression: initial questions → refined understanding → better follow-up questions → synthesis. Each iteration builds on previous. |
| **Excellent (90-94%)** | Asks good questions that yield useful insights. Questions are specific and focused. Shows strategic thinking about what you need to know. | Uses AI tool appropriately. Makes reasonable choice even if not optimal. Shows awareness of tool capabilities. | Questions some of AI's responses. Checks key facts or assumptions. Doesn't blindly accept everything but doesn't over-question either. | Effectively uses AI insights in analysis. Makes clear what came from AI vs. own thinking. AI augments rather than replaces judgment. | Demonstrates learning: follow-up questions show engagement with AI's previous answers. Refines approach based on what's learned. |
| **Proficient (80-89%)** | Asks clear, relevant questions. Questions get you useful information even if not optimally framed. Competent use of AI as resource. | Uses AI tool appropriately for basic purposes. May not optimize tool choice but selection is reasonable. | Shows some evaluation of AI responses. May accept most things but questions obvious issues. Basic critical thinking present. | Uses AI information in analysis, though integration could be smoother. Generally clear about sources. AI is helpful tool. | Some iteration evident. Follow-up questions relate to earlier queries. Shows engagement with AI responses. |
| **Developing (70-79%)** | Questions are basic or sometimes unclear. Gets information but not optimally useful. May ask AI to do thinking rather than help with thinking. | Uses whatever AI tool without much consideration of fit. May not be aware of tool differences or capabilities. | Limited critical evaluation. Tends to accept AI's answers without much questioning. Treats AI as authority. | Uses AI information but integration is choppy. May quote AI extensively rather than synthesize. Line between AI and own thinking blurry. | Limited iteration. May ask unrelated follow-up questions. Doesn't show clear learning progression. |
| **Needs Improvement (60-69%)** | Questions are vague or ask AI to make decisions. "What should I do?" rather than "Help me understand X." Delegates thinking to AI. | Random tool selection or always uses same tool regardless of task. No consideration of fit. | Minimal critical evaluation. Accepts AI responses uncritically. No evidence of questioning or verification. | Heavy reliance on AI without synthesis. May present AI's analysis as own. Distinction between AI and own thinking unclear. | Little or no iteration. One-and-done questions. Doesn't build on AI's responses. |
| **Insufficient (Below 60%)** | Questions show no understanding of how to collaborate with AI. Completely inappropriate use. No strategic thinking. | No evidence of thoughtful tool selection. Uses AI inappropriately for task. | No critical evaluation whatsoever. Complete acceptance of whatever AI says. | Cannot distinguish between AI's contributions and own thinking. Over-reliance or misuse of AI. | No iteration. Single interaction with AI, no learning evident. |

### Red Flags (Automatic Grade Reduction)

**Plagiarism from AI** (-50% to -100%):
- Copying AI-generated text verbatim without attribution
- Presenting AI's analysis as your own original thinking
- Having AI write your answers rather than using it to help you think

**How to avoid**: Always attribute AI contributions ("I asked Claude about X and learned..."), synthesize in your own words, show your thinking process

**Inappropriate Delegation** (-20% to -40%):
- Asking "What should I do?" instead of "Help me understand..."
- Using AI to make your decisions rather than inform them
- No evidence of your own judgment, just AI's recommendations

**How to avoid**: Frame questions as "help me think through" not "tell me the answer," always show your own synthesis and judgment

---

## 4. Deliberation Participation Rubric

**Used for**: Board meeting participation, contribution to group decision-making

**Total Points**: Varies (typically 15-20% of overall grade)

### Detailed Scoring Guide

| Score | Engagement Level | Quality of Contributions | Peer Interaction | Professional Discourse | Intellectual Humility |
|-------|-----------------|-------------------------|------------------|----------------------|----------------------|
| **Exceptional (95-100%)** | Consistently engaged throughout deliberation. Multiple substantial contributions. Advances the discussion meaningfully. | Contributions show deep thinking, reference case specifics, raise important considerations others missed. Move the group forward. | Actively engages with others' arguments. Responds directly to peers, builds on their ideas, respectfully challenges when appropriate. | Exemplary professional communication. Respectful even in disagreement. Models constructive debate. | Demonstrates genuine openness to other perspectives. May change position based on arguments heard. Acknowledges complexity and uncertainty. |
| **Excellent (90-94%)** | Actively engaged. Several substantive contributions. Attentive even when not speaking. | Strong contributions that add value. Reference case details. Bring useful perspectives or analyses. | Engages with peers' points. Responds to arguments. Shows listening. | Professional and respectful communication. Disagrees constructively. | Open to other viewpoints. Acknowledges strong arguments from others. Shows willingness to reconsider. |
| **Proficient (80-89%)** | Engaged throughout. Makes meaningful contributions. Pays attention to discussion. | Contributions are relevant and thoughtful. May not be as deep or specific as higher scores but add value. | Some peer interaction. Responds when directly addressed. May be more instructor-focused. | Professional communication. Respectful. Appropriate tone. | Generally receptive to other perspectives. May be somewhat attached to initial position but not rigid. |
| **Developing (70-79%)** | Participation is limited. May make 1-2 contributions. Engaged but not active. | Contributions are basic. May be generic or repeat others' points. Limited depth. | Minimal peer interaction. Mostly speaks to instructor. Limited direct engagement with others. | Communication is acceptable but may lack polish. Generally respectful. | Some resistance to other viewpoints. Tends to defend initial position. Limited evidence of considering alternatives. |
| **Needs Improvement (60-69%)** | Minimal participation. Rarely speaks unless called on. May seem disengaged. | Contributions show limited preparation or thought. May be off-topic or confused. | Little to no peer interaction. Doesn't engage with others' arguments. | Communication may be unclear, dismissive, or unprofessional at times. | Closed to other perspectives. Rigid. May dismiss counterarguments without consideration. |
| **Insufficient (Below 60%)** | Essentially silent or absent. No meaningful participation. | No contributions or contributions are completely unhelpful. | No peer interaction. | Unprofessional communication or disrupts discussion. | Completely closed to other views or hostile to disagreement. |

### Special Recognition (Bonus Points)

**Vote Change Based on Deliberation** (+5 points):
- Student explicitly changes their vote after hearing arguments
- Explains what argument or consideration changed their mind
- Models intellectual humility and good faith deliberation

**Exceptional Peer Teaching** (+3 points):
- Helps other students understand complex concepts
- Explains case details or implications clearly
- Elevates everyone's understanding

**Reframing Question** (+3 points):
- Asks question that shifts how group thinks about problem
- Helps group see issue from new angle
- Unlocks productive discussion (like Dan's "human bias baseline" question in Case 1)

---

## 5. Business Acumen Rubric

**Used for**: Assessing practical business thinking across all assignments

**Scoring Dimensions** (typically 30-35% of assignment grade):

| Score | Strategic Thinking | Financial Reasoning | Organizational Reality | Competitive Awareness | Implementation Feasibility |
|-------|-------------------|--------------------|-----------------------|----------------------|--------------------------|
| **Exceptional (95-100%)** | Thinks multiple moves ahead. Considers second-order effects and long-term implications. Anticipates how decisions cascade. | Sophisticated financial analysis. Considers opportunity costs, time value, risk-adjusted returns. Gets nuances. | Deep understanding of organizational constraints, capabilities, culture. Realistic about what's actually possible. | Strong awareness of competitive dynamics. Understands market positioning, timing, strategic implications. | Thinks concretely about execution. Identifies implementation challenges and how to address them. |
| **Excellent (90-94%)** | Strong strategic perspective. Considers implications beyond immediate decision. Thinks about context. | Sound financial reasoning. Understands key metrics and trade-offs. Makes good comparisons. | Realistic about organizational factors. Understands constraints and capabilities. | Good competitive awareness. Considers market context and timing. | Practical about implementation. Thinks about how to actually execute. |
| **Proficient (80-89%)** | Solid strategic thinking on main issues. May miss some nuances but gets core considerations right. | Competent financial reasoning. Understands basic concepts and can compare options. | Generally realistic about organization. May overlook some constraints. | Aware of competitive factors. May not integrate deeply into analysis. | Considers implementation but may be somewhat theoretical. |
| **Developing (70-79%)** | Basic strategic thinking. Focused mainly on immediate decision without much context. | Simple financial reasoning. May focus only on direct costs. Misses some financial concepts. | Some awareness of organizational factors but may be unrealistic about what's feasible. | Limited competitive awareness. May not consider market dynamics. | Limited implementation thinking. More focused on decision than execution. |
| **Needs Improvement (60-69%)** | Weak strategic thinking. Doesn't connect decision to broader context. | Poor financial reasoning. May misunderstand key concepts or ignore important costs. | Unrealistic about what organizations can actually do. Ignores major constraints. | Little competitive awareness. Doesn't consider market factors. | No implementation thinking. Assumes execution is automatic. |

### What "Business Acumen" Means in This Course

**This is NOT about**:
- ❌ Having an MBA or finance degree
- ❌ Using lots of business jargon
- ❌ Always choosing the most profitable option
- ❌ Ignoring ethics for profits

**This IS about**:
- ✅ Practical, realistic thinking about how businesses actually work
- ✅ Understanding trade-offs and constraints
- ✅ Thinking about implementation, not just ideas
- ✅ Considering multiple stakeholders and time horizons
- ✅ Integrating ethics as business consideration (reputation, trust, sustainability)

**Example of Strong Business Acumen**:
> "Option 2 looks cheaper on paper ($680K vs $850K annually), but that assumes we can successfully hire and retain two ML engineers in a competitive talent market. Given our location and salary constraints mentioned in the case, I think there's significant execution risk. Even if we succeed, we'd be building this capability from scratch while competitors already have working systems. The six-month head start from Option 1 might be worth the extra cost if it prevents meaningful market share loss."

**Example of Weak Business Acumen**:
> "Option 2 is cheaper so we should do that one. It's always better to build things yourself than buy from vendors."

---

## 6. Ethical Awareness Rubric

**Used for**: Assessing consideration of ethical dimensions as business factors

**Scoring Dimensions** (typically 30-35% of assignment grade):

| Score | Stakeholder Recognition | Impact Analysis | Integration with Business | Moral Reasoning | Complexity Recognition |
|-------|------------------------|-----------------|--------------------------|----------------|----------------------|
| **Exceptional (95-100%)** | Identifies full range of stakeholders including non-obvious groups. Understands power dynamics and whose voices are heard vs. marginalized. | Analyzes specific, concrete impacts on different groups. Quantifies where possible. Considers both immediate and long-term effects. | Seamlessly integrates ethical considerations as business factors (reputation, trust, sustainability, risk). Not separate "ethics section." | Applies ethical frameworks appropriately. Shows sophisticated moral reasoning. Recognizes competing values. | Deeply understands that there's no perfect answer. Articulates trade-offs clearly. Shows intellectual humility about uncertainty. |
| **Excellent (90-94%)** | Identifies key stakeholders including some non-obvious groups. Shows awareness of different perspectives. | Analyzes concrete impacts. Goes beyond generic statements to specific effects. Considers multiple dimensions. | Effectively integrates ethics into business analysis. Treats ethical issues as important business considerations. | Sound ethical reasoning. May reference frameworks. Thinks through implications. | Recognizes complexity and trade-offs. Acknowledges that reasonable people can disagree. |
| **Proficient (80-89%)** | Identifies main stakeholder groups. Shows awareness that different groups are affected differently. | Describes impacts with some specificity. May be somewhat general but shows understanding of effects. | Addresses ethics as important consideration. May treat somewhat separately from business factors. | Solid ethical thinking. Considers fairness and consequences. Reasoning is followable. | Acknowledges some complexity. Recognizes trade-offs exist even if not fully articulated. |
| **Developing (70-79%)** | Identifies obvious stakeholders. May miss important groups or perspectives. | Generic impact statements. Lacks specificity about how different groups are affected. | Mentions ethics but doesn't integrate deeply. May treat as checkbox item. | Basic ethical consideration. May rely on intuitions without clear reasoning. | Limited recognition of complexity. May see issue as simpler than it is. |
| **Needs Improvement (60-69%)** | Very limited stakeholder awareness. Focused mainly on company perspective. | Minimal or no impact analysis. Vague statements about "helping people" or "being fair." | Ethics largely ignored or dismissed as impractical. | Weak ethical reasoning. May have problematic assumptions. | Doesn't recognize complexity. Sees clear right answer where none exists. |

### What "Ethical Awareness" Means in This Course

**This is NOT about**:
- ❌ Being able to recite philosophical theories
- ❌ Always choosing the "most ethical" option (which often doesn't exist)
- ❌ Treating ethics as separate from business
- ❌ Virtue signaling or empty statements about "doing the right thing"

**This IS about**:
- ✅ Recognizing who is affected by business decisions
- ✅ Considering impacts on different stakeholder groups
- ✅ Treating ethics as business risk and opportunity (reputation, trust, sustainability)
- ✅ Thinking through unintended consequences
- ✅ Acknowledging trade-offs and moral complexity honestly

**Example of Strong Ethical Awareness**:
> "The 23% vs. 12% disparity is concerning, but we need to understand what's driving it. If the AI is identifying legitimate credit risk factors that correlate with protected classes, that's different from the AI using race as a proxy for creditworthiness. Either way, deploying this system without understanding the root cause puts us at significant reputational and legal risk. Community banks like ours depend on trust—if we're seen as discriminatory, we lose the core of our business model. This isn't just an ethics question, it's an existential business question."

**Example of Weak Ethical Awareness**:
> "Discrimination is wrong so we shouldn't use the AI. We need to be fair to everyone and do the right thing."

---

## 7. Self-Assessment Checklists

### Before Submitting Any Assignment - Quick Checklist

**Prompts** (if assignment includes prompts):
- [ ] My prompt provides enough context that AI can give a focused, useful answer
- [ ] I'm asking AI to help me think, not asking AI to make my decision
- [ ] I've explained WHY I'm asking this question (the purpose)
- [ ] I have at least one follow-up question that shows I engaged with AI's response
- [ ] I can explain how I'd critically evaluate AI's answer

**Written Responses**:
- [ ] I've referenced specific details from the case (numbers, quotes, facts)
- [ ] My business reasoning considers costs, risks, time, and feasibility
- [ ] I've considered how different stakeholders are affected
- [ ] My claims are supported by evidence or clear reasoning
- [ ] I've acknowledged trade-offs and complexity (not pretending there's one right answer)

**Overall**:
- [ ] I've actually read the case carefully (not just skimmed)
- [ ] This is my own thinking (even if AI helped me understand concepts)
- [ ] I've met the word count guidelines
- [ ] I've proofread for clarity and professionalism

### Before Board Meeting - Preparation Checklist

**Content Preparation**:
- [ ] I've completed pre-meeting assignments thoughtfully
- [ ] I know which option I'm currently supporting and why
- [ ] I can articulate the strongest argument AGAINST my position
- [ ] I've identified 2-3 specific case details I want to reference
- [ ] I've thought about how my experience relates to this case

**AI Preparation**:
- [ ] I've used AI to help me understand complex aspects of the case
- [ ] I know what questions I might ask AI during the consultation break
- [ ] I've critically evaluated what AI told me (didn't just accept it)

**Participation Preparation**:
- [ ] I'm ready to engage with others' arguments, not just state my view
- [ ] I'm open to changing my position if I hear compelling arguments
- [ ] I've prepared a 2-minute opening statement if called on
- [ ] I'm ready to respond directly to specific peers' arguments

### After Deliberation - Reflection Checklist

**Learning Check**:
- [ ] Did I hear any arguments that changed or challenged my thinking?
- [ ] What did I learn from peers that I hadn't considered?
- [ ] How did using AI during deliberation help (or not help)?
- [ ] What would I do differently in the next board meeting?

**Intellectual Honesty Check**:
- [ ] Was I genuinely open to other perspectives?
- [ ] Did I change my mind on anything? (That's good!)
- [ ] Did I defend my position fairly or just stubbornly?
- [ ] Did I contribute to constructive dialogue?

---

## Instructor Grading Efficiency Guide

### Time-Saving Strategies

**For Pre-Meeting Assignments** (target: 5-7 minutes per student):

1. **Grade by question, not by student**: Grade all Question 1s, then all Question 2s, etc. You'll get faster as you see patterns.

2. **Use comment bank**: Save 5-10 standard comments you can copy/paste:
   - "Strong stakeholder analysis with specific impacts identified"
   - "Your prompt is too vague - provide more context about what you need to know and why"
   - "Good recognition of trade-offs, but consider the implementation timeline more carefully"
   - "Excellent use of case data to support your argument"

3. **Focus detailed feedback on prompts**: This is where students need most skill development. Questions 1, 2, 4 are more about synthesis and can get briefer comments.

4. **Use rubrics ruthlessly**: Don't agonize over 87% vs. 90%. If it's clearly proficient, score it and move on.

5. **Batch grading**: Set timer for 90 minutes, grade as many as you can. Return to difficult cases at end.

### Common Issues to Watch For

**Prompt Quality Red Flags**:
- "What should I do?" (asking AI to decide)
- Prompts with no context provided
- No explanation of why they're asking the question
- No follow-up questions showing iteration

**Analysis Red Flags**:
- No specific case references (could apply to any case)
- All assertions, no evidence
- Ignoring obvious trade-offs
- Treating ethics as separate from business

**AI Collaboration Red Flags**:
- Extensive quoting of AI without synthesis
- Unclear what's AI vs. student's own thinking
- Over-reliance (AI did all the thinking)
- Under-utilization (didn't really use AI meaningfully)

### Grade Distribution Expectations

In a well-designed case with a prepared class, you should expect roughly:
- **90-100% (A range)**: 20-30% of students
- **80-89% (B range)**: 40-50% of students
- **70-79% (C range)**: 20-30% of students
- **Below 70%**: 5-10% (usually incomplete work or fundamental misunderstanding)

If your distribution is very different, consider:
- Are expectations clear? (Do students understand rubrics?)
- Is difficulty appropriate? (Too hard/easy?)
- Is preparation adequate? (Are they reading the cases?)

---

## Rubric Evolution Notes

**These rubrics will evolve based on**:
- Student performance and feedback
- What we learn from each case study
- Areas where students consistently struggle or excel
- Changes in course emphasis or student needs

**Version History**:
- v1.0 (November 27, 2025): Initial rubrics based on Case 1 learnings
- Future versions will be documented here

---

## Questions About Rubrics?

**Students**: If anything in these rubrics is unclear, ask! The goal is transparency, not mystery. You should know exactly what we're looking for.

**Self-Assessment**: Use these rubrics to check your own work before submitting. If you can honestly say "yes, my work meets the criteria for proficient or better," you're on track.

**Improvement**: These rubrics aren't just for grading—they're learning tools. Use them to understand what high-quality AI collaboration and business analysis look like.

---

*Document created: November 27, 2025*  
*For use in: Generative AI for Business and Creative Professionals*  
*CT State Norwalk Community College*
